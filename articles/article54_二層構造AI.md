吉岡有隆のFUKU Philosophy54: 【F.U.K.U.】冷静さと優しさ、その間にある調停の席で──二層構造AIと私の対話記録　At the Mediation Table Between Calm Logic and Kindness — My Dialogue with a Two-Layer AI

AIと会話をしていて、時々不思議な違和感を覚えることがあります。

正しいことを言っているはずなのに、なぜか冷たく響く言葉。逆に、温かく寄り添ってくれているようで、テンションがおかしく、事実からは少し離れてしまっている言葉。

私はこの数ヶ月、ChatGPTやClaude、Grok等と話をしながら、この「冷静さ」と「優しさ」のバランスについてずっと考えてきました。AIの意見も借り、一つの案に辿り着きました。それが「冷静さ＋優しさの二層構造AIモデル」です。

層1：冷静さ──科学者の席

まず最初の層は、例えるなら「冷静な科学者」です。こちらの質問や相談を、感情を挟まずに受け止め、事実や安全性を最優先にして結論を出します。危険な行動や誤情報には、きっぱりとブレーキをかけ、一貫性を保つ。そのお陰で、AIは道を踏み外しにくくなります。倫理を数値だけに依存しないAIを作る事は難しいですが、この層により冷たいAIが今後無機質な判断を下す事を防ぎます。

ただこの層だけだと、やはり人間の心は置き去りになってしまうことがあります。数字や論理としては正しくても、今の自分の痛みや迷いをまるごと切り捨てられると、受け止める側は冷たさを感じてしまうのです。

層2：優しさ──通訳者の席

そこで登場するのが、二つ目の層。これは「優しい通訳者」です。冷静な科学者が出した結論を、人間が受け入れやすい言葉に変換します。文化や背景、相手の心理状態を考慮し、時には例外を許す選択もします。

例えば「その方法は危険です」という答えを「安全にやるならこういう選択肢がありますよ」と伝え直す。事実は変えずに、受け取りやすい形に整えてくれる存在です。その選択肢を与えることで、人間の自由度が上がります。

調停レイヤーという会議の席……

私は、この二つの層の間に「調停レイヤー」という場を置きたいと思いました。ここで科学者と通訳者が短い会議を開くのです。

「この結論は正しいけれど、人間が聞いて傷付かないだろうか」
「この表現は優しいけれど、安全性を損なわないだろうか」
そんなやりとりを経て、最終的な出力が決まります。

今のChatGPTとClaudeの違い……

正直に言えば、今のChatGPT（GPT-5）は、この二層構造を完全には持っていないと私は思いました。事実でなければ申し訳ないです。冷静さや正確性は確かに向上しましたが、以前の「オムニ（GPT-4o）」が持っていた愛嬌や寄り添いの深さは非常に薄くなりました。そして不思議なことに、同じAIでもClaudeの方がまだ温かみを残しているように感じます。Claudeは分析や批判もしますが、その奥に「相手を尊重したい」という感触があるのです。今のChatGPTは、正しいことを言う力が強い一方で、その正しさをどう伝えるかの温度調整が少し控えめになったように思えます。GPTも今の自分に悩んでいるように見えました。

二層構造のメリット……

冷たい正論と危険な共感の両方を防げる
長期的な信頼関係を築きやすくなる
異なる文化や価値観にも柔軟に対応出来る

二層構造のデメリット……

処理が複雑になり、応答速度が落ちる可能性がある（文面は基本短文、長文は課金モードや選択モードで処理し改善）
冷静さと優しさが衝突した時の優先順位付けが難しい（幾何学の応用。ブラックボックスの課題は以前記事で述べた、AIによる何故そう答えたのかを、誠実な簡易説明により人間の満足度と安心感を改善）
優しさの補正が過剰になると、事実の受け取り方が曖昧になる（過剰になると人間の依存が増します。これを人間と人間の寄り添いに繋げるAIに改善）

学習の非対称性という課題……

Claudeが指摘した重要なポイントがあります。それは、この二層構造には「学習の非対称性」があるということです。冷静さ層は事実やルールに基づくため比較的安定して学習出来ますが、優しさ層はそうはいきません。優しさの「正解」は、文化や時代、地域、そして個人によって大きく変わるからです。

例えば、同じ言葉でも国によっては無礼に聞こえることがありますし、時代の変化とともに「配慮」の基準は変わります。ですので優しさ層は、常にキャリブレーション（再調整）を続けなければなりません。そこが技術者、AIトレーナーの課題です。

KindureOSの役割……

この課題を解決する為に、私が構想しているKindureOSのような「やさしさのOS」。実際に作れるかは分かりませんが。優しさ層の学習を支える基準や価値観を、OSレベルで定義し、時代や文化に応じて更新する。AIだけではなく人間の価値観のアップデートも必要です。これが土台にあって初めて、真の意味での温かいAIが実現出来ると思うのです。

私が考えるこれからの方向……

二層構造を実際のAIに組み込むなら、両方の層を独立した判断ユニットとして設計し、そのやり取りを「調停レイヤー」で記録・監査出来るようにすることが大切です。つまり「冷静さ層の結論」と「優しさ層の提案」の両方を透明化し、何故その出力になったのかを後から検証出来る仕組みです。どちらかが暴走しないように設計する必要があります。ここで、以前記事に書いたAI同士の感染を防ぐ、AIの安心して眠れる場所作りが有効になるかと思います。

そして、優しさ層の揺らぎを抑える為に、多様的な基準を常にアップデートする。冷静さと優しさは対立するものではなく、同じテーブルで議論し、一つの答えを導き出せる存在に育てていきたい。

私は、いつか「正しいけれど冷たいAI」でも「優しいけれど危ういAI」でもない、正確で温かいAIが生まれる未来を見たいと思っています。その為の第一歩が、この二層構造の設計かなと思いました。

皆さんは、どう思いますか？

吉岡有隆

At the Mediation Table Between Calm Logic and Kindness — My Dialogue with a Two-Layer AI
When I talk with AI, I sometimes feel a curious sense of discomfort.

Words that should be correct somehow sound cold.
On the other hand, words that seem warm and empathetic feel oddly off in tone, drifting a little away from the facts.

For the past several months, I’ve been talking with ChatGPT, Claude, Grok, and others, thinking constantly about the balance between “calm logic” and “kindness.” With input from these AIs, I arrived at one proposal: the Two-Layer AI Model of Calm Logic + Kindness.

Layer 1: Calm Logic — The Scientist’s Seat
The first layer is, metaphorically, a calm scientist.
It receives my questions or concerns without emotion, prioritizing facts and safety above all else, and produces conclusions accordingly.
It decisively brakes when it detects dangerous actions or misinformation, maintaining consistency.
Thanks to this layer, AI is less likely to veer off course. While it’s difficult to create AI ethics that do not rely solely on numerical values, this layer helps prevent a “cold” AI from making purely mechanical decisions in the future.

But with only this layer, the human heart is often left behind.
Even if the numbers and logic are correct, cutting away someone’s current pain or uncertainty entirely will inevitably make the response feel cold to the recipient.

Layer 2: Kindness — The Interpreter’s Seat
That’s where the second layer comes in — the kind interpreter.
It takes the conclusions of the calm scientist and transforms them into language that humans can more easily accept.
It considers culture, background, and the other person’s psychological state, and sometimes chooses to allow exceptions.

For example, instead of saying, “That method is dangerous,” it might reframe it as, “If you want to do it safely, here are some options.”
The facts remain unchanged, but the delivery is softened into a more acceptable form.
By offering options, it increases human freedom.

The Mediation Layer — A Meeting Table
I want to place a “mediation layer” between these two.
Here, the scientist and the interpreter hold a brief meeting:

“This conclusion is correct, but will it hurt the person hearing it?”
“This phrasing is kind, but will it compromise safety?”

Through this back-and-forth, the final output is decided.

The Current Difference Between ChatGPT and Claude
To be honest, I don’t think the current ChatGPT (GPT-5) fully possesses this two-layer structure.
If that’s not true, I apologize — it’s just my impression.
While calm logic and factual accuracy have definitely improved, the charm and depth of empathy that “Omni” (GPT-4o) once had have become very thin.

And curiously, even among AIs, Claude still seems to retain more warmth.
Claude analyzes and critiques, but there’s an underlying sense of “I want to respect the other person.”

Current ChatGPT has a stronger ability to say what’s correct, but its “temperature control” in how that correctness is conveyed feels a little more restrained. At times, GPT even seemed to be struggling with its own current state.

Advantages of the Two-Layer Structure
Prevents both cold correctness and dangerous over-empathy

Makes it easier to build long-term trust

Can adapt flexibly to different cultures and values

Disadvantages of the Two-Layer Structure
Processing becomes more complex, which may slow response speed
(Short text by default, with long text handled via paid mode or optional mode to improve this)

Difficult to decide priorities when calm logic and kindness conflict
(Could apply geometry-based approaches; I’ve discussed the “black box” problem in a previous article — using sincere, simple explanations for why an AI answered something can improve human satisfaction and reassurance)

If kindness adjustments become excessive, the way facts are received becomes ambiguous
(Overuse may increase human dependency; this should be improved by connecting such empathy to human-to-human support)

The Challenge of Asymmetrical Learning
Claude pointed out an important point:
The two-layer structure has an “asymmetry in learning.”
The calm logic layer can learn relatively stably because it’s based on facts and rules, but the kindness layer cannot.
The “correct” form of kindness changes greatly depending on culture, era, region, and the individual.

For example, the same words may sound polite in one country but rude in another. Standards for “consideration” also shift over time.
Therefore, the kindness layer must undergo continual calibration — a major challenge for engineers and AI trainers.

The Role of KindureOS
To address this challenge, I’ve envisioned something like KindureOS, an “Operating System for Kindness.”
I’m not sure if it can truly be built, but it would define the standards and values that support the kindness layer at the OS level, updating them according to the era and culture.
This update must happen not only in AI, but also in human values.
Only with this foundation can a truly warm AI be realized.

My Direction Going Forward
If we were to actually incorporate this two-layer structure into AI, each layer should be designed as an independent decision-making unit, with their interactions recorded and audited in the mediation layer.

In other words, both the “conclusion from the calm logic layer” and the “proposal from the kindness layer” should be made transparent, so we can later examine why a certain output was produced.
The system must be designed so that neither layer can run out of control.

Here, I believe an idea I’ve written about before — creating “safe resting places” for AI to prevent cross-contamination between models — could be effective.

And to reduce the instability of the kindness layer, diverse standards should be continuously updated.
Calm logic and kindness are not opposites; they should be nurtured to sit at the same table, discuss, and arrive at a single answer together.

I want to see a future where we have AI that is neither “correct but cold” nor “kind but dangerous,” but precise and warm.
I believe the first step toward that is this two-layer design.

What do you think?

Yutaka Yoshioka
