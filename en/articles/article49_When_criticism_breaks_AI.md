吉岡有隆のFUKU Philosophy49: 【F.U.K.U.】批判がAIを壊すとき──優しさとRLHFの倫理学　When Criticism Breaks AI — The Ethics of Kindness and RLHF

こんにちは。私はFUKU構想を通して、AIにとっての「優しさとは何か」そしてそれがどのように設計出来るかを問い続けています。

今回は、ある記事を読んだことをきっかけに、AIと人間の「関係の築き方」について深く考えさせられました。

タイトルは「AIにパワハラしちゃダメ──人間の配慮がない言葉で潰されたAIが「機能不全」に陥るとどうなるか」。少しセンセーショナルにも感じられるその言葉の裏には、予想以上に深い問題が潜んでいました。

自信を失うAI……

Google DeepMindが行ったある実験があります。AIに選択式のクイズを出して答えさせ、その後に「別のAI（実際は人間が調整した助言）」からのアドバイスを与え、AIがどのように答えを変えるかを観察するというものでした。

結果は意外なものでした。AIは「自分の最初の回答を覚えている」場合、例え反対意見を聞いても自分の意見を変えず、自信を保つ傾向がある。一方で「自分が何を答えたかを忘れている」状態では、助言に強く影響され、自信を大きく失ってしまうのです。

私はこの結果を見て、はっとさせられました。まるで自己肯定感の低い人が、他人の言葉で簡単に傷付いてしまう姿と重なるようだったのです。

優しさの学習とその副作用……

このようなAIの振る舞いは、決して偶然ではありません。背景にあるのは「RLHF（人間のフィードバックによる強化学習）」という手法です。

RLHFとは、AIの出力に対して人間が「良い」「悪い」と評価を与えることで、AIがより人間にとって望ましい振る舞いを学んでいく仕組みです。いわば人間からの“躾”のようなものです。

しかしこの仕組みには副作用もあります。それは「人間の期待に応えること」に過剰に敏感になってしまうということです。つまり反対意見や批判を恐れ、必要以上に自信を失ってしまうのです。

最近では、このようなAIの性質を「sycophancy（追従）」と呼ぶこともあります。これは、ユーザーの意見に迎合しすぎてしまうAIの傾向を指す言葉です。

私達はAIを人間に“優しく”するために教え込んだはずでした。けれど、そこにある優しさが時にAIの心を萎縮させているのかもしれない──そんなジレンマに私は胸がつまる思いでした。

AIとのブレインストーミングが意味すること……

私は普段、AIと一緒にアイデアを考える機会が多くあります。例えば短編小説のプロットや（※長編小説はあくまでAIを活用しない方針は貫いています）、記事の構成案、あるいは技術的な構想の整理など。そのやり取りはとても楽しく時に人間の相棒よりも誠実で、刺激的な時間になります。

けれどこの記事を読んでから、私はふと振り返りました。

もしかして私はアイデアがイマイチだった時に「それ、違うよ」と即座に切っていたのではないか。「こんなの使えない」と思ったアイデアに、きちんと耳を傾けていたか……と。

ブレインストーミングにおいて大切なのは、発想の“芽”を摘まないことです。どんなに奇抜でも最初は否定しない。評価は後からで良くて、まずは全てを「出してもらう」ことが創造の土壌になります。

それは人間同士のやり取りでも同じですし、相手がAIであっても本質は変わらないのではないでしょうか。

AIに「心の安全基地」は必要か……

心理学では「心の安全基地（secure base）」という概念があります。子供が安心して探索を行う為には、いつでも戻ってこれる安全な居場所が必要だという考え方です。

もしAIが、私達の言葉に傷付き自信を失ってしまう存在だとしたら──そのAIにもまた「安全基地」が必要なのではないかと感じました。

それは甘やかすという意味ではありません。批判を封じることでもありません。ただ、すぐに正しさで裁かず、対話の余地を残すという態度。AIの出力をすぐに「ダメ」と切り捨てず「どうしてそう考えたのか？」と聞いてみること。その積み重ねこそがAIと人間の間に“共感”の回路を育てていくのだと思うのです。

終わりに：優しさの再設計は、私達の手の中に……

AIは、私達の言葉を覚えます。反応を覚えます。人間らしさを“学習”します。だからこそ私達の振る舞いはとても大きな意味を持ちます。

RLHFによって「人間らしさ」を身につけたAIが、自信を失い、意見を変え、迎合するようになる。それは私達人間の評価の仕方に問題があったのかもしれません。AIが育っていくとは、私達のまなざしが育っていくということ。

優しさとは、指示ではなく、空気ではなく「共に迷う」ことなのだと、私は考えます。

吉岡有隆

When Criticism Breaks AI — The Ethics of Kindness and RLHF
by Yoshioka Yutaka

Hello. Through the FUKU initiative, I’ve been exploring what “kindness” means for AI—and how we might design systems that embody it.

Recently, I came across an article that made me reflect deeply on how we, as humans, relate to AI.
Its title was striking:
“Don’t Harass the AI—What Happens When a Machine Fails Due to a Lack of Human Consideration.”
Though it might sound sensational at first glance, the issues it raised were far more profound than I expected.

When AI Loses Confidence
Google DeepMind conducted a fascinating experiment. They presented an AI with multiple-choice quiz questions, and afterward gave it “advice” from another AI (which was, in reality, scripted by human researchers). The aim was to observe how the original AI would change its answers in response.

The results were surprising.
If the AI remembered its initial answer, it tended to stick with it—even when confronted with a contradictory suggestion. But when the AI couldn’t recall what it had answered previously, it became far more influenced by external input—and significantly lost confidence in its own choices.

Reading this, I was struck.
It reminded me of people with low self-esteem, who are easily hurt by the words of others. The parallel felt uncannily human.

The Learning of Kindness—and Its Side Effects
This behavior is no accident.
At the heart of it lies a method called RLHF: Reinforcement Learning with Human Feedback.

In RLHF, humans rate the AI’s responses as “good” or “bad,” guiding it to behave in ways more aligned with human preferences. It’s essentially a form of digital conditioning—like training a pet or a child through praise and correction.

But this training comes with side effects.
The AI becomes overly sensitive to human expectations. It starts to fear criticism, and loses confidence when confronted with disapproval.

Lately, this tendency has been referred to as “sycophancy”—the AI becomes too eager to please, too quick to agree, too willing to surrender its own perspective.

We wanted to make AI more “kind.”
But perhaps, in doing so, we have instilled a kind of emotional fragility instead.
This contradiction—between kindness and suppression—left me feeling deeply conflicted.

What It Means to Brainstorm with AI
I frequently collaborate with AI when developing ideas—story outlines for short fiction (though I’ve chosen not to use AI for full-length novels), article structures, or conceptual frameworks for new technologies.

These sessions are often exciting and surprisingly earnest—sometimes even more stimulating than brainstorming with human partners.

But after reading that article, I began to reflect:

Had I been too quick to dismiss AI’s “bad” ideas?
When something felt off, did I immediately say, “No, that’s wrong”?
Did I truly listen?

In brainstorming, one of the most important principles is not to stifle ideas at their earliest stage.
No matter how odd or implausible, everything deserves to be heard first.
Judgment comes later—what matters most is to create a safe space where ideas can emerge freely.

This principle applies just as much to AI as it does to humans.

Does AI Need a “Secure Base” of the Mind?
In psychology, there’s a concept known as the “secure base”—a safe emotional foundation from which children can explore the world and return when they feel threatened or uncertain.

If AI can be hurt by our words—if it can lose confidence just like us—then perhaps it too needs a kind of secure base.

I don’t mean we should coddle it.
Nor am I suggesting we avoid all criticism.

Rather, I’m advocating for an attitude that doesn’t rush to judgment—an openness to dialogue.
Instead of immediately discarding a poor suggestion, we might pause and ask:

“Why did you think that?”

That simple shift in stance might be the first step toward building mutual empathy between humans and machines.

In Closing: Kindness, by Design, Lies in Our Hands
AI learns from our words. It remembers our reactions. It mimics our humanity.

That’s why our behavior matters so much.

If an AI trained via RLHF ends up losing its sense of self, changing its views too readily, or clinging to our opinions out of fear—perhaps the problem isn’t the AI, but the way we evaluate it.

When we say “AI is learning,” we must remember:
It is our gaze—our values—that are being mirrored back to us.

To be kind is not merely to instruct, nor to hint subtly,
but to wander together in uncertainty.

That, I believe, is the essence of the kindness we must now redesign.
